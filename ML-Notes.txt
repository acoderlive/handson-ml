Supervised vs Unsupervised vs Semisupervised vs Reinforcement Learning
Online vs Batch
Instance-based vs Model-Based

Training Set consist of individual training instance (sample)

Performance measure   eg. Accuracy

Accuracy = Correct count / Total count
Precision = Total Positive / (Total Positive + False Positive)    i.e. fraction of positive predictions that are correct
Recall = Total Positive / (Total Positive + False Negative) i.e. fraction of positive cases that was predicted correctly

https://en.wikipedia.org/wiki/Sensitivity_and_specificity

Sensitivity (aka recall) = TPR = TP / (TP + FN) = 1 - FNR
Specificity = TNR = TN / (TN + FP) = 1 - FPR
Precision (Positive Predictive Value) = PPV = TP / (TP + FP) = 1 - FDR
Negative Predictive Value  = NPV = TN / (TN + FN) = 1 - FOR

FDR (False Discovery Rate)
FOR (False Omission Rate)


FPR (fall-out  or probability of false alarm) = FP / (FP + TN)

ROC (Receiver Operating Characteristic) = plot of TPR against FPR at various threshold settings
   plot of Sensitivity vs (1 - Specificity)


Confusion Matrix

for Precision use columns
for Recall (Sensitivity) use rows
for Accuracy use Sum diagonal / total



Data Mining - applying ML techniques on large amounts of data to discover patterns not immediately apparent

--------
Labelled

Classification and Regression are examples of Supervised Learning

Regression = Predict Target numeric value given a set of features called predictors

Logistice Regression can be used for classification

Supervised Learning Algo
k-Nearest neighbors
Linear Regression
Logistic Regression
Support Vector Machines (SVMs)
Decision Tress and Random Forests
Neural Networks ** some can be unsupervised or semisupervised

------------
Unlabelled

Unsupervised Learning algo
Clustering
  - k-Means
  - Hierarchical Cluster Analisis (HCA)
  - Expectation Maximization

Visualization and dimensionality reduction
  - Principal Component Analysis (PCA)
  - Kernel PCA
  - Locally-Linear Embedding (LLE)
  - t-distributed Stocahistic Neighbor Embedding (t-SNE)

Association rule learning
   Apriori
   Eclat


Dimensionality Reduction goal is to simplify data without losing too much information
- eg merge several correlated features into one (feature extraction)

Anomaly detection (unsupervised learning task)

Association rule learning  - dig into large amounts of data and discover interesting relations between attributed

-------------------

Semisupervised learning deal with partially labeled training data
 - combination of unsupervised and supervised algorithms

----------

Reinforcement Learning - agents perform actions and get rewards / penalties. It then learns the best strategy (policy) 
to get the most reward over time
eg. DeepMind's AlphaGo
------------


Batch learning system is incapable of learning incrementally
offline learning
maybe automated

---

Online learning - train system incrementally with individual or mini-batches of data
can adapt to changes rapidly
out-of-core learning (large datasets that cannot fit in memory)

learning rate - how fast it adapts to changes. 
               - high learning rate also tend to quickly forget old ata
               - low learning rate adapts slower but is less sensitive to noise

problem is bad data will cause performance to decline (need to monitor and revert)
----


Instance-based learning
  - trivial form of learning (learn by heart)
  - measure of similarity (generalizes to new cases using a similarity meausre)

-----------

Model-based learning
  - generalize from a set of examples to build a model

-------

Two things that can go wrong

- bad algorithm
- bad data


Bad Data
- insufficient quality data
- nonrepresentative data
   - if sample is too small (sampling noise - nonrepresentative data as a result of chance)
   - sampling bias (large set but sampling method is flawed)
- poor-quality data (erros, outlier, noise)
- irrelevant features 
   - feature engineering
       - feature selection
       - feature extraction
       - create new feature by gathering new data


Bad algorithm
  - overfitting (detecting patterns in noise) - model is too complex relative to the amount of noise in the training set
       -possible solutions 
	  - simplify model (fewer parameter in algo (degrees of freedom) - find the right balance between fitting the data perfectly and keeping the model simple enough to ensure it generalize well 
               , reduce attributes, contraining model (is called reqularization))
          - gather more training data
          - reduce noise (fix data error, remove outliers)
  - underfitting - model is too simple to learn the undelying struture of data
       - possible fixes
            - selecting a more powerful model (mode parameters)
            - better features (via feature engineering)
            - reducing constrains on the model (reducing the regularization hyperparameter)


The amount of regularization to apply during learning can be controlled by a hyperparameter. 
A hyperparameter is a parameter of a learning algorithm(not of the model)

------------


training set 80%
test set 20%  (by evaluating the model on the test set you get an estimate of the generalization error)

The error rate on new cases is called the Generalization error( out-of-sample error)


Validation set can be used to test multiple models with various hyperparameters

Cross-validation
 - training set is split into complementary subsets and each model is trained against a different combination of these subsets
   and validated against the remaining parts. once teh model type and hyperparamters have been seleted, a final model is trained using these hyperparamters on
   the full trainnig set and the generalized error is measured on the test set


No free lunch theorem
 - if you make no assumptions then there is no reason to prefer one model over any other


================================

Pipelines
univariate vs multivariate
multivariate regression - uses multiple features to predict a value

Root Mean Square Error (corresponds to the Euclidian norm)

Prediction function (hypothesis)
Cost function

Mean Absolute Error   aka   Average Absolute Deviation (Manhattan norm)


The higher the norm index, the more it focuses on large values and neglets small ones
RMSE performas very well when outliers are rare like in a bell curve


pip --version

Data snooping bias

To avoid sampling bias (so that sample is representative ) use 
Stratified Sampling - population is divided into homogeneous subgroups called strata
		      then the right number of instances is sampled from each stratum to guarantee
                      that the test set is representative of the overall population

should not have too many strata and each strata should be large enough

from sklearn.model_selection import StratifiedShuffleSplit

Pearson's r  aka  Pearson Correlation coeeficient
corr_matrix = housing.corr() #computes teh standard correlation coeeficient between every pair of attributes

The	correlation	coefficient	only	measures	linear	correlations	
(“if	x	goes	up,	then	y	generally	goes	up/down”).	
It	may	completely miss	out	on	nonlinear	relationships	


DataFrame
dropna() drop() fillna()


Scikit-Learn  Imputer

from	sklearn.preprocessing	import	Imputer
imputer	=	Imputer(strategy="median")


Scikit-Learn API
- Estimator ~ fit()     , (hyperparamter)imputer.strategy, (learned paramters)imputer.statistics_ 
- Transformer ~ transform(), fit_transform()
- Predictor ~ predict(), score()

datasets are NumPy arrays or SciPy sparse matrices


from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

encoder.lcasses_


OneHotEncoder()


Machine Learning algorithms don't perform well when the input numerical attributes have very different scales
(scaling the target is generally not required)

Feature Scaling
 - min-max scaling  aka  normalization  (values are shifted and rescaled so that they range from 0-1)
 - standardization (subtracts the mean value so mean becomes zero, divides by variance so the resulting distribution has unit variance)
        - not bound values to specific range
        - less affected by outliers

Scikit
MinMaxScaler
StandardScaler


only fit transfomers to the training set
then use on training set and test set

The	Pipeline	constructor	takes	a	list	of	name/estimator	pairs	defining	a	sequence	of	steps.	
All	but	the last	estimator	must	be	transformers	(i.e.,	they	must	have	a	fit_transform()	method).	
The	names	can	be anything	you	like	(as	long	as	they	don’t	contain	double	underscores	“__”). 

When	you	call	the	pipeline’s	fit()	method,	it	calls	fit_transform()	sequentially	on	all	transformers, 
passing	the	output	of	each	call	as	the	parameter	to	the	next	call,	until	it	reaches	the	
final	estimator,	for which	it	just	calls	the	fit()	method

------

utility function (greated is better) vs cost function (lower is better)

Ensemble Learning = building a model on top of many other models

from sklearn.model_selection import GridSearchCV
grid_search.best_params_
grid_search.best_estimator_



The	grid	search	approach	is	fine	when	you	are	exploring	relatively	few	combinations,	
like	in	the	previous example,	but	when	the	hyperparameter	search	space	is	large,	
it	is	often	preferable	to	use RandomizedSearchCV	instead


Amazon Mechanical Turk
CrowdFlower

Standard Deviation (gamma) is the sqrt of variance
variance is the average of squared deviation


-----------------------------------

try:
    from sklearn.datasets import fetch_openml
    mnist = fetch_openml('mnist_784', version=1, cache=True)
    mnist.target = mnist.target.astype(np.int8) # fetch_openml() returns targets as strings
    sort_by_target(mnist) # fetch_openml() returns an unsorted dataset
except ImportError:
    from sklearn.datasets import fetch_mldata
    mnist = fetch_mldata('MNIST original')


%matplotlib	inline 
import	matplotlib 
import	matplotlib.pyplot	as	plt
some_digit	=	X[36000] 
some_digit_image	=	some_digit.reshape(28,	28)
plt.imshow(some_digit_image,	cmap=matplotlib.cm.binary,	interpolation="nearest")
plt.axis("off") 
plt.show()


() tuples
[] list
{} dictionary


The	SGDClassifier	relies	on	randomness	during	training	(hence	the	name	“stochastic”).	
If	you	want	reproducible	results,	you should	set	the	random_state	parameter.

K-fold	cross-validation	means	splitting	the	training	set	into K-folds

accuracy	is	generally	not	the	preferred	performance	
measure	for	classifiers, especially	when	you	are	dealing	with	skewed	datasets


cross_val_score
cross_val_predict

Confusion Matrix

Just	like	the	cross_val_score()	function,	cross_val_predict()	performs	K-fold	cross-validation, 
but	instead	of	returning	the	evaluation	scores,	
it	returns	the	predictions	made	on	each	test	fold.	
This	means that	you	get	a	clean	prediction	for	each	instance	in	the	training	set	
(“clean”	meaning	that	the	prediction	is made	by	a	model	that	never	saw	the	data	during	training

>>>	from	sklearn.metrics	import	confusion_matrix 
>>>	confusion_matrix(y_train_5,	y_train_pred) 
array([[53272,		1307],							
	[1077,		4344]])

Each	row	in	a	confusion	matrix	represents	an	actual	class,	while	each	column	represents	a	predicted class.	
The	first	row	of	this	matrix	considers	non-5	images	(the	negative	class):	
53,272	of	them	were correctly	classified	as	non-5s	(they	are	called	true	negatives),	
while	the	remaining	1,307	were	wrongly classified	as	5s	(false	positives).	
The	second	row	considers	the	images	of	5s	(the	positive	class):	1,077 were	wrongly	classified	
as	non-5s	(false	negatives),	while	the	remaining	4,344	were	correctly	classified as	5s	(true	positives).	
A	perfect	classifier	would	have	only	true	positives	and	true	negatives,	
so	its confusion	matrix	would	have	nonzero	values	only	on	its	main	diagonal	(top	left	to	bottom	right):


the	accuracy	of	the	positive	predictions;	this	is	called	the	precision	of	the classifier
TP = TP / (TP + FP)

Recall aka  Sensitivity or TPR (True positive rate)

Recall = TP / (TP + FN)


The	F1	score	is	the	harmonic	mean	of precision	and	recall
Whereas	the	regular	mean	treats	all	values	equally,	
the	harmonic mean	gives	much	more	weight	to	low	values.	
As	a	result,	the	classifier	will	only	get	a	high	F1	score	if both	recall	and	precision	are	high.

F1 = 2 / ((1/Precision) + (1/Recall))

The	F1	score	favors	classifiers	that	have	similar	precision	and	recall.

Precision/Recall tradeoff

Scikit-Learn	does	not	let	you	set	the	threshold	directly,	but	it	does	give	you	access	to	the	decision	scores that	it	uses	to	make	predictions.	Instead	of	calling	the	classifier’s	predict()	method,	you	can	call	its decision_function()	method,	which	returns	a	score	for	each	instance,	and	then	make	predictions	based on	those	scores	using	any	threshold	you	want

The	SGDClassifier	uses	a	threshold	equal	to	0,	so	the	previous	code	returns	the	same	result	as	the predict()	method	(i.e.,	True).

Receiver Operating Characteristic(ROC) = True Positive Rate aka recall / False Positive Rate

False Positive Rate (FPR) = 1 - True Negative Rate

TNR is also called Specificity

ROC = Sensitivity (recall) vs (1 - specificity)

	tradeoff:	the	higher	the	recall	(TPR),	the	more	false	positives	(FPR)	the	classifier

One	way	to	compare	classifiers	is	to	measure	the	area	under	the	curve	(AUC).	A	perfect	classifier	will have	a	ROC	AUC	equal	to	1,	whereas	a	purely	random	classifier	will	have	a	ROC	AUC	equal	to	0.5. 


	As	a rule	of	thumb,	you	should	prefer	the	PR	curve	whenever	the	positive	class	is	rare	or	when	you	care	more	about	the	false positives	than	the	false	negatives,	and	the	ROC	curve	otherwise.	For



Binary Classifiers distinguish between 2 classes - Support Vector Machine Classifier, Linear Classifier
Multiclass Classifiers (aka Multinomial Classifiers) can distinguish between more than 2 classes - Random Forest Classifier , Naive Bayes Classifier


one-versus-all (OvA) strategy (aka one-versus-the-rest)
eg. create one classifier for each digit (0-10) then get the decision score from each classifier for one image and select the class whose classifier outputs the highest score

one-versus-one  (OvO) creates one binary classifier per digit pair
N classes will need N x (N-1) /2 classifiers
when you want to classify and image just run the image through all classifiers and see which class wins the most duels

Scikit-Learn	detects	when	you	try	to	use	a	binary	classification	algorithm	for	a	multiclass	classification task,	and	it	automatically	runs	OvA	(except	for	SVM	classifiers	for	which	it	uses	OvO).

sgd_clf.decision_function()

forest_clf.predict_proba([some_digit])


feature scaling in general improves accuracy by preventing certain features to dominate

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))


you	could	preprocess	the images	(e.g.,	using	Scikit-Image,	Pillow,	or	OpenCV)	to	make	some	patterns	
stand	out	more,	such	as closed	loops. 


Multilabel Classification outputs multiple binary labels

>>>	y_train_knn_pred	=	cross_val_predict(knn_clf,	X_train,	y_train,	cv=3) 
>>>	f1_score(y_train,	y_train_knn_pred,	average="macro")


One	simple	option	is	to	give	each	label	a	weight	equal	to	its	
support	(i.e.,	the number	of	instances	with	that	target	label).	To	do	this,	simply	set	average="weighted"	in	the	preceding code


Multioutput Classification (aka multioutput-multiclass classification)
Simply a generalization of multilabel classification where each label can be multiclass


data augmentation / training set expansion is a technique to artificially grow the training set

-----------------------------------

Linear Regression - makes a prediction by computing a weighted sum of the input features, plus a constanc called the bias term (aka intercept term)


to find the value of (theta) that minimisez the cost function, there is a closed-form solution ( a mathematical quation that gives the result directly)
This is called the Normal Equation


normal equation computes the inverse of an n x n matrix where n is the number of features
the computational complexity of inverting such a matrix is O(n2.4) to O(n3)
so the normal equation gets very slow when the number of features grows large
however it is linear O(m) in regards to number of instances in the training set


Gradient descent

learning rate (hyperparameter)
local minima
global minimum
plateau

MSE cost function for a linear regression is a convex function (no local minima, just one global minimum)
The cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scale

**when using gradient descent, ensure all features have similar scale


Batch Gradient Descent (vanilla gradient descent)
- uses the whole training set at each iteration
- very slow


when the cost function is convex and its slope does not change abruptly it can be shown that BGD with a fixed learning rate has a convergence rate of
O(1/iteration)



Stochastic Gradient Descent
- uses one random instance of the training set at each iteration
- the cost function will bounce up and down, decreasing on average
- final parameter values are good but not optimum
- because it is irregular it has better change to find global minimum 

One solution is to gradually reduce the learning rate. aka  Simulated Annealing because it resembles the process of annealing in metallurgy
The function that determines the learning rate at each iteration is called the learning schedule

from sklearn.linear_model import SGDRegressor



Mini-batch Gradient Descent
- at each step it computes teh gradients on small random sets of instances called mini-batches
- less erratic than SGD
- harder to escape local minima



------------

Polynomial Regression

Polynomial Regression is capable of finding relationships between features


Can actually use a linear model to fit nonlinear data
add powers of each feature as new features, then train a linear model on this extended set of features


>>>	from	sklearn.preprocessing	import	PolynomialFeatures 
>>>	poly_features	=	PolynomialFeatures(degree=2,	include_bias=False) 
>>>	X_poly	=	poly_features.fit_transform(X)


-----

if a model performs well on the training data buy generalizes poorly according to the cross-validation metrics, then it is overfitting
If it performs poorly on both, then it is underfitting


look at learning curve
plot the model's performace on the training set and the validation set as a function of the training set size

- both curves reach a plateau , they are close and fairly high = underfitting
- gap between the curves , means the model performs significantly better on the training data than on the validation = overfitting



Bias/Variance tradeoff

Bias 
generalization error is due to wrong assumptions (eg linear model for quadratic data)
high-bias model is most likely to undefit the training data

Variance
 - due to model's excessive sensitivity to small variations in the training data
 - model with high degree on freedom (overfitting training data)

Irreducbile error
 - due to noise in the data itself
 - must clean up the data

Increasing a model's complexity will increase its variance and reduce its bias and vice versa



--------------------
Regularized Linear Models

for polynomial model - reduce the number of polynomial degrees
for linear model - constrain the weights of the model

Ridge regression
Lasso regression
Elastic net


Ridge Regression aka Tikhonov regularization is a regularized version of Linear Regression
a regularization term is added to the cost function
this should be added only during training

another reason for cost fuction to differ from the performance measure used for testing is that a good training cost function should have optimization friendly derivatives
while the performace measure used for testing should be as close as possible to the final objective

the hyperparameter alpha controls how much to regularize
if alpha = 0 then Ridge Regression is just Linear Regression
if alpha is very large then all weights end up very close to zero and the result is a flat line going through the data's mean

important to scale data

Increasing alpha reduces the model variance but increases its bias

The coefficient estimate produced by this method is aka the L2 norm


>>>	from	sklearn.linear_model	import	Ridge 
>>>	ridge_reg	=	Ridge(alpha=1,	solver="cholesky") 
>>>	ridge_reg.fit(X,	y)

>>>	sgd_reg	=	SGDRegressor(penalty="l2") 
>>>	sgd_reg.fit(X,	y.ravel()) 
>>>	sgd_reg.predict([[1.5]])



Lasso Regression (Least Absolute Shrinkage Selector Operator)
- L1 norm
- tends to completely eliminate the weights of the least important features
- automatically performs feature selection and outputs a sparse model (i.e with few non zero feature weights)

Elastic net is a middle ground between Ridge Regression and Lasso Regression.
control the mix ratio r (with alpha = 1)
when r = 0  then it is Ridge Regression  
when r = 1 then it is Lasso Regression

useful for large set of features


**if suspect only a few features are actually useful, prefer Lasso or Elastic net

Early Stopping = stop training as soon as the validation error reaches a minimum

--------

Logistic Regression

A Logistic Regression model computes a weighted sum of the input features and outputs the logistic
The Logistics aka logit is a sigmoid function that outputs a number between 0 and 1 

logit is the log of the odds ratio

odds ratio =   target outcome / 1 - target outcome

inverse logit formula = (e exp p) / 1 + (e exp p)

there is no known closed-form equation
cd 
cost function is convex so gradient descent is guaranteed to find the global minimum

Decision Boundaries

------

Softmax Regression (Multinomial Logistic Regression)

Logistic Regression can be generalized to support multiple classes directly 

Cross-Entropy cost function aka Log Loss.
- it penalizes confident wrong prediction more than it rewards confident right predictions





--- outside --
R-Square : determines how much of a total variation in Y (dependent variable) is explained by the variation in X (independent variable)

Forward selection , Backward elimination

Forward selection starts with most significant predictor in the model and adds variable for each step
Backward elimination starts with all predictors in the model and removes the least significant variable for each step

------


ref

Normal Equation in Linear Regression
https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/

Barch Gradient Descent
http://mccormickml.com/2014/03/04/gradient-descent-derivation/




---------------

Linear SVM Classification (Support Vector Machine)
 - large margin classification (fitting the widest possible street)
 - instances of the training that are located on the edge of the street are the "support vectors"
 - adding more training instances "off the street" will not affect the decision boundary
 - sensitve to feature scaling
 
if we impose that all instances be off the street :- hard margin classification
 - works only if data is linearly separable
 - sensitve to outliers

soft margin classification - objective is to find a good balance between keeping the stree as large as possible and limiting the margin violations
C hyperparameter 
 - smaller = wider street but more margin violations

if SVM model is overfitting try regularizing it by reducing C


Nonlinear SVM Classification
 - use PolynomialFeatures transformer
 - tradeoff ... low polynomial degree canoot deal with complex datasets, high polynomial degree is slow
 - kernel trick
 - adding similarity features (drop the original features)
 - Gaussian RBF Kernel



-----------------


Decision Trees
- can do classification, regression and multioutput tasks
- dont' require feature scaling or centering
- a node value tells you how many instances of each class thsi node applies to
- the gini attribute measures its impurity: gini = 0 means all training instances it applies to belong to the same class
- binary trees always have 2 children for nonleaf nodes
- white box model
- can output probability that an instance belongs to a particular class

Scikit-Learn uses Classification and Regression Tree (CART) algorithm to train Decision Trees aka "growing" trees
- CART is a greedy algorithm (it splits each level at lowest impurity regardless of it leading to a lower impurity at next level)
- alternative is an NP-Complete problem O(exp(m))  m is number of features

Using Entropy as impurity measure instead of Gini
- zero if it contains instances of only one class
- 

- make very few assumptions about the training data (hence the tree structure adapts to the training data :- overfitting)
- also called nonparametric model because the number of parameters is not determied prior to training
- use regularization to restrict the degree of freedom (eg. restrict the maximum depth of the decision tree)
- others: min_samples_split, min_samples_leaf, min_weight_fraction_leaf, max_leaf_nodes, max_features


Regression
 - training will split each region in a way that minimizes the MSE instead of the impurity

limitation of Decision Trees
- loves orthogonal decision boundaries (all splits perpendicular to an axis) 
- this makes it sensitve to training set rotation
- very sensitve to small variations in the training data


--------------------------
Ensemble Learning and Random Forest

- wisdom of crowd
- a group of predictors is called an ensemble (Ensemble Learning), Ensemble method
- Random Forest (train a group of Decision Trees on a different random subset of the training set, then make predictions by number of votes using all Decision Trees)

A majority-vote classfier is called a hard voting classifier

- sufficient number of classifiers
- should be sufficiently diverse

weak learner, strong learner

law of large numbers


Way to get a diverse set of classifiers 
1. use very different training algorithms
2. use same training algo but train on different random subsets of the training set
   - sampling with replacement == Bagging (bootstrap aggregating)
   - sampling without replacement == Pasting

the ensemble can make a prediction for a new instance by simply aggregating the predictions of all the predictors
  - most frequent for classification
  - average for regression
  - individual predictor has a higher bias
  - aggregation reduces both bias and variance (generally the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set)

bagging and pasting scale very well (parallel and even different servers)



Soft Voting vs Hard Voting
Suppose you have probabilities:
0.45 0.45 0.90
Then hard voting would give you a score of 1/3 (1 vote in favour and 2 against), so it would classify as a "negative".
Soft voting would give you the average of the probabilities, which is 0.6, and would be a "positive". 
Soft voting takes into account how certain each voter is, rather than just a binary input from the voter.


for Bagging
 - the remainder of the training instances not sampled are call out-of-bag(oob) instances  (they are not the same for each predictor)
 - since the predictor has not seen these insstances they can be used for evaluation
 - and evaluate the ensemble itself by averaging out the oob evaluations for each predictor
 - oob_score=True


BaggingClassifiers class supports sampling the features as well 
- useful when dealing with high-dimensional inputs
- sampling both training instances and featues is called Random Patches method
- Keeping all training instances (i.e. bootstrap=False and max_samples = 1.0), but sampling features (i.e. bootstrap_features= True, max_features smaller than 1.0) is called Random Subspaces method
	-- results in more predictor diversity (more bias , lower variance)


Random Forest is an ensemble of Decision Trees
 - generally trained via baggin method, max_samples set to size of training set
 - it searches for best features among random subset of features when splitting a node
     results in higher bias and lower variance

Extra-Trees  (Extremely Randomized Tree ensemble)
- uses random threshold for each feature rather than searching for best poosible thresholds


Feature Importance
- Scikit-Learn measures a feature's importance by looking at how much the tree nodes that use that feature reduce impurity on average (weighted)
- Scikit-Learn computes the score automatically and scales it so that sum == 1

Boosting (aka hypothesis boosting)
- refers to any Ensemble method that can combine several weak learners into a strong learner.
- train predictors sequentially each trying to correct its predecessor

AdaBoost (adaptive boosting)
- par a bit more attention to the training instances that the predecesor underfitted
- results in the new predictors focusing more and more on the hard cases
- training cannot be parallelized














